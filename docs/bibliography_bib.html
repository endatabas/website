<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>

<head>
<title>bibliography.bib</title>
<link rel="stylesheet" href="/resources/css/dark.css" media="(prefers-color-scheme: dark)" />
<link rel="stylesheet" href="/resources/css/light.css" media="(prefers-color-scheme: light)" />
<link rel="stylesheet" href="/resources/css/bib_bib.css" />
</head>

<body>
<h1>bibliography.bib</h1><pre>
@comment{{This file has been generated by bib2bib 1.99}}
</pre>

<pre>
@comment{{Command line: bib2bib -ob bibliography.bib references.bib research.bib}}
</pre>

<pre>
@comment{{Prefer citations from https://dl.acm.org/, if available. DOI as citekey, if available.}}
</pre>

<a name="10.1145/318898.318923"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/318898.318923">10.1145/318898.318923</a>,
  author = {Copeland, George P. and Khoshafian, Setrag N.},
  title = {A Decomposition Storage Model},
  year = {1985},
  isbn = {0897911601},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/318898.318923},
  doi = {10.1145/318898.318923},
  booktitle = {Proceedings of the 1985 ACM SIGMOD International Conference on Management of Data},
  pages = {268–279},
  numpages = {12},
  location = {Austin, Texas, USA},
  series = {SIGMOD '85}
}
</pre>

<a name="10.1109/69.755613"></a><pre>
@article{<a href="bibliography.html#10.1109/69.755613">10.1109/69.755613</a>,
  author = {Jensen, Christian S. and Snodgrass, Richard Thomas},
  title = {Temporal Data Management},
  year = {1999},
  issue_date = {January 1999},
  publisher = {IEEE Educational Activities Department},
  address = {USA},
  volume = {11},
  number = {1},
  issn = {1041-4347},
  url = {https://doi.org/10.1109/69.755613},
  doi = {10.1109/69.755613},
  abstract = {A wide range of database applications manage time-varying information. Existing database technology currently provides little support for managing such data. The research area of temporal databases has made important contributions in characterizing the semantics of such information and in providing expressive and efficient means to model, store, and query temporal data. This paper introduces the reader to temporal data management, surveys state-of-the-art solutions to challenging aspects of temporal data management, and points to research directions.},
  journal = {IEEE Trans. on Knowl. and Data Eng.},
  month = jan,
  pages = {36–44},
  numpages = {9},
  keywords = {temporal database, SQL, time-constrained database, Query language, TSQL2, transaction time, valid time., temporal data model, user-defined time}
}
</pre>

<a name="10.5555/320037"></a><pre>
@book{<a href="bibliography.html#10.5555/320037">10.5555/320037</a>,
  author = {Snodgrass, Richard Thomas},
  title = {Developing Time-Oriented Database Applications in {SQL}},
  year = {1999},
  isbn = {1558604367},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA}
}
</pre>

<a name="10.1145/3180143"></a><pre>
@article{<a href="bibliography.html#10.1145/3180143">10.1145/3180143</a>,
  author = {Ngo, Hung Q. and Porat, Ely and R\'{e}, Christopher and Rudra, Atri},
  title = {Worst-Case Optimal Join Algorithms},
  year = {2018},
  issue_date = {June 2018},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {65},
  number = {3},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/3180143},
  doi = {10.1145/3180143},
  abstract = {Efficient join processing is one of the most fundamental and well-studied tasks in database research. In this work, we examine algorithms for natural join queries over many relations and describe a new algorithm to process these queries optimally in terms of worst-case data complexity. Our result builds on recent work by Atserias, Grohe, and Marx, who gave bounds on the size of a natural join query in terms of the sizes of the individual relations in the body of the query. These bounds, however, are not constructive: they rely on Shearer’s entropy inequality, which is information-theoretic. Thus, the previous results leave open the question of whether there exist algorithms whose runtimes achieve these optimal bounds. An answer to this question may be interesting to database practice, as we show in this article that any project-join style plans, such as ones typically employed in a relational database management system, are asymptotically slower than the optimal for some queries. We present an algorithm whose runtime is worst-case optimal for all natural join queries. Our result may be of independent interest, as our algorithm also yields a constructive proof of the general fractional cover bound by Atserias, Grohe, and Marx without using Shearer’s inequality. This bound implies two famous inequalities in geometry: the Loomis-Whitney inequality and its generalization, the Bollob\'{a}s-Thomason inequality. Hence, our results algorithmically prove these inequalities as well. Finally, we discuss how our algorithm can be used to evaluate full conjunctive queries optimally, to compute a relaxed notion of joins and to optimally (in the worst-case) enumerate all induced copies of a fixed subgraph inside of a given large graph.},
  journal = {J. ACM},
  month = mar,
  articleno = {16},
  numpages = {40},
  keywords = {Join Algorithms, fractional cover bound, Bollob\'{a}s-Thomason inequality, Loomis-Whitney inequality}
}
</pre>

<a name="10.1145/2590989.2590991"></a><pre>
@article{<a href="bibliography.html#10.1145/2590989.2590991">10.1145/2590989.2590991</a>,
  author = {Ngo, Hung Q and R\'{e}, Christopher and Rudra, Atri},
  title = {Skew Strikes Back: New Developments in the Theory of Join Algorithms},
  year = {2014},
  issue_date = {December 2013},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {42},
  number = {4},
  issn = {0163-5808},
  url = {https://doi.org/10.1145/2590989.2590991},
  doi = {10.1145/2590989.2590991},
  journal = {SIGMOD Rec.},
  month = feb,
  pages = {5–16},
  numpages = {12}
}
</pre>

<a name="10.1145/2213836.2213946"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/2213836.2213946">10.1145/2213836.2213946</a>,
  author = {Sikka, Vishal and F\"{a}rber, Franz and Lehner, Wolfgang and Cha, Sang Kyun and Peh, Thomas and Bornh\"{o}vd, Christof},
  title = {Efficient Transaction Processing in {SAP} {HANA} Database: The End of a Column Store Myth},
  year = {2012},
  isbn = {9781450312479},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2213836.2213946},
  doi = {10.1145/2213836.2213946},
  abstract = {The SAP HANA database is the core of SAP's new data management platform. The overall goal of the SAP HANA database is to provide a generic but powerful system for different query scenarios, both transactional and analytical, on the same data representation within a highly scalable execution environment. Within this paper, we highlight the main features that differentiate the SAP HANA database from classical relational database engines. Therefore, we outline the general architecture and design criteria of the SAP HANA in a first step. In a second step, we challenge the common belief that column store data structures are only superior in analytical workloads and not well suited for transactional workloads. We outline the concept of record life cycle management to use different storage formats for the different stages of a record. We not only discuss the general concept but also dive into some of the details of how to efficiently propagate records through their life cycle and moving database entries from write-optimized to read-optimized storage formats. In summary, the paper aims at illustrating how the SAP HANA database is able to efficiently work in analytical as well as transactional workload environments.},
  booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
  pages = {731–742},
  numpages = {12},
  keywords = {column store, transaction processing, sap hana},
  location = {Scottsdale, Arizona, USA},
  series = {SIGMOD '12}
}
</pre>

<a name="10.14778/3436905.3436913"></a><pre>
@article{<a href="bibliography.html#10.14778/3436905.3436913">10.14778/3436905.3436913</a>,
  author = {Li, Tianyu and Butrovich, Matthew and Ngom, Amadou and Lim, Wan Shen and McKinney, Wes and Pavlo, Andrew},
  title = {Mainlining Databases: Supporting Fast Transactional Workloads on Universal Columnar Data File Formats},
  year = {2021},
  issue_date = {December 2020},
  publisher = {VLDB Endowment},
  volume = {14},
  number = {4},
  issn = {2150-8097},
  url = {https://doi.org/10.14778/3436905.3436913},
  doi = {10.14778/3436905.3436913},
  abstract = {The proliferation of modern data processing tools has given rise to open-source columnar data formats. These formats help organizations avoid repeated conversion of data to a new format for each application. However, these formats are read-only, and organizations must use a heavy-weight transformation process to load data from on-line transactional processing (OLTP) systems. As a result, DBMSs often fail to take advantage of full network bandwidth when transferring data. We aim to reduce or even eliminate this overhead by developing a storage architecture for in-memory database management systems (DBMSs) that is aware of the eventual usage of its data and emits columnar storage blocks in a universal open-source format. We introduce relaxations to common analytical data formats to efficiently update records and rely on a lightweight transformation process to convert blocks to a read-optimized layout when they are cold. We also describe how to access data from third-party analytical tools with minimal serialization overhead. We implemented our storage engine based on the Apache Arrow format and integrated it into the NoisePage DBMS to evaluate our work. Our experiments show that our approach achieves comparable performance with dedicated OLTP DBMSs while enabling orders-of-magnitude faster data exports to external data science and machine learning tools than existing methods.},
  journal = {Proc. VLDB Endow.},
  month = feb,
  pages = {534–546},
  numpages = {13}
}
</pre>

<a name="DBLP:conf/cidr/BonczZN05"></a><pre>
@inproceedings{<a href="bibliography.html#DBLP:conf/cidr/BonczZN05">DBLP:conf/cidr/BonczZN05</a>,
  title = {{MonetDB/X100}: Hyper-Pipelining Query Execution},
  author = {Boncz, Peter A. and Zukowski, Marcin and Nes, Niels},
  booktitle = {Second Biennial Conference on Innovative Data Systems Research, {CIDR}
               2005, Asilomar, CA, USA, January 4-7, 2005, Online Proceedings},
  pages = {225--237},
  publisher = {www.cidrdb.org},
  year = {2005},
  url = {<a href="http://cidrdb.org/cidr2005/papers/P19.pdf">http://cidrdb.org/cidr2005/papers/P19.pdf</a>},
  timestamp = {Mon, 18 Jul 2022 17:13:00 +0200},
  biburl = {https://dblp.org/rec/conf/cidr/BonczZN05.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
</pre>

<a name="10.1145/3035918.3056101"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/3035918.3056101">10.1145/3035918.3056101</a>,
  author = {Verbitski, Alexandre and Gupta, Anurag and Saha, Debanjan and Brahmadesam, Murali and Gupta, Kamal and Mittal, Raman and Krishnamurthy, Sailesh and Maurice, Sandor and Kharatishvili, Tengiz and Bao, Xiaofeng},
  title = {{Amazon} {Aurora}: Design Considerations for High Throughput Cloud-Native Relational Databases},
  year = {2017},
  isbn = {9781450341974},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3035918.3056101},
  doi = {10.1145/3035918.3056101},
  abstract = {Amazon Aurora is a relational database service for OLTP workloads offered as part of Amazon Web Services (AWS). In this paper, we describe the architecture of Aurora and the design considerations leading to that architecture. We believe the central constraint in high throughput data processing has moved from compute and storage to the network. Aurora brings a novel architecture to the relational database to address this constraint, most notably by pushing redo processing to a multi-tenant scale-out storage service, purpose-built for Aurora. We describe how doing so not only reduces network traffic, but also allows for fast crash recovery, failovers to replicas without loss of data, and fault-tolerant, self-healing storage. We then describe how Aurora achieves consensus on durable state across numerous storage nodes using an efficient asynchronous scheme, avoiding expensive and chatty recovery protocols. Finally, having operated Aurora as a production service for over 18 months, we share the lessons we have learnt from our customers on what modern cloud applications expect from databases.},
  booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
  pages = {1041–1052},
  numpages = {12},
  keywords = {recovery, quorum models, databases, log processing, performance, replication, distributed systems, oltp},
  location = {Chicago, Illinois, USA},
  series = {SIGMOD '17}
}
</pre>

<a name="ISO/IEC-9075-2:1999"></a><pre>
@techreport{<a href="bibliography.html#ISO/IEC-9075-2:1999">ISO/IEC-9075-2:1999</a>,
  author = {ISO/IEC 9075-2:1999},
  title = {Information technology — Database languages — {SQL} — Part 2: Foundation ({SQL/Foundation})},
  note = {https://www.iso.org/standard/26197.html},
  year = {1999},
  month = dec,
  type = {Standard},
  institution = {ISO/IEC}
}
</pre>

<a name="ISO/IEC-9075-2:2003"></a><pre>
@techreport{<a href="bibliography.html#ISO/IEC-9075-2:2003">ISO/IEC-9075-2:2003</a>,
  author = {ISO/IEC 9075-2:2003},
  title = {Information technology — Database languages — {SQL} — Part 2: Foundation ({SQL/Foundation})},
  note = {https://www.iso.org/standard/34133.html},
  year = {2003},
  month = dec,
  type = {Standard},
  institution = {ISO/IEC}
}
</pre>

<a name="ISO/IEC-19075-2:2021"></a><pre>
@techreport{<a href="bibliography.html#ISO/IEC-19075-2:2021">ISO/IEC-19075-2:2021</a>,
  author = {ISO/IEC 19075-2:2021},
  title = {Information technology — Guidance for the use of database language {SQL} — Part 2: Time-related information},
  note = {https://www.iso.org/standard/78933.html},
  year = {2021},
  month = aug,
  type = {Standard},
  institution = {ISO/IEC}
}
</pre>

<a name="10.1145/3318464.3380579"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/3318464.3380579">10.1145/3318464.3380579</a>,
  author = {Nathan, Vikram and Ding, Jialin and Alizadeh, Mohammad and Kraska, Tim},
  title = {Learning Multi-Dimensional Indexes},
  year = {2020},
  isbn = {9781450367356},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3318464.3380579},
  doi = {10.1145/3318464.3380579},
  abstract = {Scanning and filtering over multi-dimensional tables are key operations in modern analytical database engines. To optimize the performance of these operations, databases often create clustered indexes over a single dimension or multi-dimensional indexes such as R-Trees, or use complex sort orders (e.g., Z-ordering). However, these schemes are often hard to tune and their performance is inconsistent across different datasets and queries. In this paper, we introduce Flood, a multi-dimensional in-memory read-optimized index that automatically adapts itself to a particular dataset and workload by jointly optimizing the index structure and data storage layout. Flood achieves up to three orders of magnitude faster performance for range scans with predicates than state-of-the-art multi-dimensional indexes or sort orders on real-world datasets and workloads. Our work serves as a building block towards an end-to-end learned database system.},
  booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
  pages = {985–1000},
  numpages = {16},
  keywords = {in-memory, multi-dimensional, indexing, databases, primary index},
  location = {Portland, OR, USA},
  series = {SIGMOD '20}
}
</pre>

<a name="YouTube-YjAVsvYGbuU"></a><pre>
@misc{<a href="bibliography.html#YouTube-YjAVsvYGbuU">YouTube-YjAVsvYGbuU</a>,
  author = {Råberg, Håkan},
  title = {The Design and Implementation of a Bitemporal {DBMS}},
  url = {https://www.youtube.com/watch?v=YjAVsvYGbuU},
  year = {2019},
  month = sep,
  keywords = {temporal, bitemporal, z-curves},
  location = {Helsinki, Finland},
  series = {ClojuTRE 2019}
}
</pre>

<a name="YouTube-Px-7TlceM5A"></a><pre>
@misc{<a href="bibliography.html#YouTube-Px-7TlceM5A">YouTube-Px-7TlceM5A</a>,
  author = {Råberg, Håkan},
  title = {Light and Adaptive Indexing for Immutable Databases},
  url = {https://www.youtube.com/watch?v=Px-7TlceM5A},
  year = {2022},
  month = sep,
  keywords = {machine learning, adaptive indexes, databases, indexing, separation of storage from compute},
  location = {St. Louis, MO, USA},
  series = {Strange Loop 2020}
}
</pre>

<a name="10.14778/3415478.3415534"></a><pre>
@article{<a href="bibliography.html#10.14778/3415478.3415534">10.14778/3415478.3415534</a>,
  author = {Liu, Zhen Hua and Hammerschmidt, Beda and McMahon, Doug and Chang, Hui and Lu, Ying and Spiegel, Josh and Sosa, Alfonso Colunga and Suresh, Srikrishnan and Arora, Geeta and Arora, Vikas},
  title = {Native {JSON} Datatype Support: Maturing {SQL} and {NoSQL} Convergence in {Oracle} {Database}},
  year = {2020},
  issue_date = {August 2020},
  publisher = {VLDB Endowment},
  volume = {13},
  number = {12},
  issn = {2150-8097},
  url = {https://doi.org/10.14778/3415478.3415534},
  doi = {10.14778/3415478.3415534},
  abstract = {Both RDBMS and NoSQL database vendors have added varying degrees of support for storing and processing JSON data. Some vendors store JSON directly as text while others add new JSON type systems backed by binary encoding formats. The latter option is increasingly popular as it enables richer type systems and efficient query processing. In this paper, we present our new native JSON datatype and how it is fully integrated with the Oracle Database ecosystem to transform Oracle Database into a mature platform for serving both SQL and NoSQL style access paradigms. We show how our uniquely designed Oracle Binary JSON format (OSON) is able to speed up both OLAP and OLTP workloads over JSON documents.},
  journal = {Proc. VLDB Endow.},
  month = sep,
  pages = {3059–3071},
  numpages = {13}
}
</pre>

<a name="10.1145/2588555.2595628"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/2588555.2595628">10.1145/2588555.2595628</a>,
  author = {Liu, Zhen Hua and Hammerschmidt, Beda and McMahon, Doug},
  title = {{JSON} Data Management: Supporting Schema-Less Development in {RDBMS}},
  year = {2014},
  isbn = {9781450323765},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2588555.2595628},
  doi = {10.1145/2588555.2595628},
  abstract = {Relational Database Management Systems (RDBMS) have been very successful at managing structured data with well-defined schemas. Despite this, relational systems are generally not the first choice for management of data where schemas are not pre-defined or must be flexible in the face of variations and changes. Instead, No-SQL database systems supporting JSON are often selected to provide persistence to such applications. JSON is a light-weight and flexible semi-structured data format supporting constructs common in most programming languages. In this paper, we analyze the way in which requirements differ between management of relational data and management of JSON data. We present three architectural principles that facilitate a schema-less development style within an RDBMS so that RDBMS users can store, query, and index JSON data without requiring schemas. We show how these three principles can be applied to industry-leading RDBMS platforms, such as the Oracle RDBMS Server, with relatively little effort. Consequently, an RDBMS can unify the management of both relational data and JSON data in one platform and use SQL with an embedded JSON path language as a single declarative language to query both relational data and JSON data. This SQL/JSON approach offers significant benefits to application developers as they can use one product to manage both relational data and semi-structured flexible schema data.},
  booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
  pages = {1247–1258},
  numpages = {12},
  keywords = {JSON, no-SQL, schema-less, XML, SQL/XML, SQL/JSON},
  location = {Snowbird, Utah, USA},
  series = {SIGMOD '14}
}
</pre>

<a name="Petkovi2017SQLJSONSP"></a><pre>
@article{<a href="bibliography.html#Petkovi2017SQLJSONSP">Petkovi2017SQLJSONSP</a>,
  title = {{SQL/JSON} Standard: Properties and Deficiencies},
  author = {Du{\v s}an Petkovi{\'c}},
  journal = {Datenbank-Spektrum},
  year = {2017},
  volume = {17},
  pages = {277-287}
}
</pre>

<a name="Chamberlin2019ComparingSQLPlusPlusSQL2016"></a><pre>
@article{<a href="bibliography.html#Chamberlin2019ComparingSQLPlusPlusSQL2016">Chamberlin2019ComparingSQLPlusPlusSQL2016</a>,
  title = {Comparing Two {SQL}-Based Approaches for Querying {JSON}: {SQL++} and {SQL:2016}},
  author = {Don Chamberlin},
  journal = {Couchbase Resources},
  year = {2019},
  month = aug,
  url = {https://info.couchbase.com/rs/302-GJY-034/images/Comparing_Two_SQL_Based_Approaches_WP.pdf}
}
</pre>

<a name="XQuery31"></a><pre>
@techreport{<a href="bibliography.html#XQuery31">XQuery31</a>,
  author = {Jonathan Robie and Michael Dyck and Josh Spiegel},
  title = {{XQuery} 3.1: An {XML} Query Language},
  month = mar,
  note = {https://www.w3.org/TR/xquery-31/},
  year = {2017},
  type = {Recommendation},
  institution = {W3C}
}
</pre>

<a name="Chamberlin2003XQuery"></a><pre>
@misc{<a href="bibliography.html#Chamberlin2003XQuery">Chamberlin2003XQuery</a>,
  author = {Don Chamberlin},
  title = {{XQuery}: A Query Language for {XML} (slides)},
  month = jun,
  year = {2003},
  url = {https://cseweb.ucsd.edu/classes/wi19/cse232B-a/papers/sigmod03_xquery.pdf},
  note = {Accessed: {2023–01-01}}
}
</pre>

<a name="10.1007/s00778-002-0074-9"></a><pre>
@article{<a href="bibliography.html#10.1007/s00778-002-0074-9">10.1007/s00778-002-0074-9</a>,
  author = {Ailamaki, Anastassia and DeWitt, David J. and Hill, Mark D.},
  title = {Data Page Layouts for Relational Databases on Deep Memory Hierarchies},
  year = {2002},
  issue_date = {November 2002},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  volume = {11},
  number = {3},
  issn = {1066-8888},
  url = {https://doi.org/10.1007/s00778-002-0074-9},
  doi = {10.1007/s00778-002-0074-9},
  abstract = {Relational database systems have traditionally optimized for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results (which were obtained without using any indices on the participating relations), when compared to NSM: (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75\% of NSM's stall time due to data cache accesses; (b) range selection queries and updates on memory-resident relations execute 1725\% faster; and (c) TPC-H queries involving I/O execute 1148\% faster. Finally, we show that PAX performs well across different memory system designs.},
  journal = {The VLDB Journal},
  month = nov,
  pages = {198–215},
  numpages = {18},
  keywords = {Disk page layout, Cache-conscious database systems, Relational data placement}
}
</pre>

<a name="CHMOralHistoryOfDonaldChamberlin"></a><pre>
@misc{<a href="bibliography.html#CHMOralHistoryOfDonaldChamberlin">CHMOralHistoryOfDonaldChamberlin</a>,
  title = {Oral History of {Donald} {Chamberlin}},
  author = {Paul McJones},
  publisher = {Computer History Museum},
  url = {https://archive.computerhistory.org/resources/access/text/2015/06/102702111-05-01-acc.pdf},
  month = jul,
  year = {2009}
}
</pre>

<a name="10.1145/262762.262770"></a><pre>
@article{<a href="bibliography.html#10.1145/262762.262770">10.1145/262762.262770</a>,
  author = {McHugh, Jason and Abiteboul, Serge and Goldman, Roy and Quass, Dallas and Widom, Jennifer},
  title = {Lore: A Database Management System for Semistructured Data},
  year = {1997},
  issue_date = {Sept. 1997},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {26},
  number = {3},
  issn = {0163-5808},
  url = {https://doi.org/10.1145/262762.262770},
  doi = {10.1145/262762.262770},
  abstract = {Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.},
  journal = {SIGMOD Rec.},
  month = sep,
  pages = {54–66},
  numpages = {13}
}
</pre>

<a name="10.1145/800083.802685"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/800083.802685">10.1145/800083.802685</a>,
  author = {Copeland, George},
  title = {What If Mass Storage Were Free?},
  year = {1980},
  isbn = {9781450373951},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/800083.802685},
  doi = {10.1145/800083.802685},
  abstract = {This paper investigates how database systems would be designed and used under the limiting-case assumption that mass storage is free. It is argued that free mass storage would free database systems from the limitations and problems caused by conventional deletion techniques. A non-deletion strategy would significantly simplify database systems and their operation, as well as increase their functionality and availability. Consideration of this limiting case helps shed light on a more realistic argument: if the cost of mass storage were low enough, then deletion would become undesirable.It is also argued that the often labor-intensive costs and time delays involved in archival and retrieval of older data can be minimized if a single technology were available with low-cost on-line storage and a low-cost archival media with long shelf life.Optical discs promise to come one to two orders of magnitude closer to the limiting case of free mass storage than ever before. Other features of optical discs include improved reliability and a single technology for both on-line and archival storage with a long shelf life. Because of these features and because of (not in spite of) their non-deletion limitation, it is argued that optical discs fit the requirements of database systems better than magnetic discs and tapes.},
  booktitle = {Proceedings of the Fifth Workshop on Computer Architecture for Non-Numeric Processing},
  pages = {1–7},
  numpages = {7},
  location = {Pacific Grove, California, USA},
  series = {CAW '80}
}
</pre>

<a name="10.1145/1013881.802685"></a><pre>
@article{<a href="bibliography.html#10.1145/1013881.802685">10.1145/1013881.802685</a>,
  author = {Copeland, George},
  title = {What If Mass Storage Were Free?},
  year = {1980},
  issue_date = {March 1980},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {15},
  number = {2},
  issn = {0163-5840},
  url = {https://doi.org/10.1145/1013881.802685},
  doi = {10.1145/1013881.802685},
  abstract = {This paper investigates how database systems would be designed and used under the limiting-case assumption that mass storage is free. It is argued that free mass storage would free database systems from the limitations and problems caused by conventional deletion techniques. A non-deletion strategy would significantly simplify database systems and their operation, as well as increase their functionality and availability. Consideration of this limiting case helps shed light on a more realistic argument: if the cost of mass storage were low enough, then deletion would become undesirable.It is also argued that the often labor-intensive costs and time delays involved in archival and retrieval of older data can be minimized if a single technology were available with low-cost on-line storage and a low-cost archival media with long shelf life.Optical discs promise to come one to two orders of magnitude closer to the limiting case of free mass storage than ever before. Other features of optical discs include improved reliability and a single technology for both on-line and archival storage with a long shelf life. Because of these features and because of (not in spite of) their non-deletion limitation, it is argued that optical discs fit the requirements of database systems better than magnetic discs and tapes.},
  journal = {SIGIR Forum},
  month = mar,
  pages = {1–7},
  numpages = {7}
}
</pre>

<a name="10.1145/800296.811515"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/800296.811515">10.1145/800296.811515</a>,
  author = {Chamberlin, Donald D. and Boyce, Raymond F.},
  title = {{SEQUEL}: A Structured {English} Query Language},
  year = {1974},
  isbn = {9781450374156},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/800296.811515},
  doi = {10.1145/800296.811515},
  abstract = {In this paper we present the data manipulation facility for a structured English query language (SEQUEL) which can be used for accessing data in an integrated relational data base. Without resorting to the concepts of bound variables and quantifiers SEQUEL identifies a set of simple operations on tabular structures, which can be shown to be of equivalent power to the first order predicate calculus. A SEQUEL user is presented with a consistent set of keyword English templates which reflect how people use tables to obtain information. Moreover, the SEQUEL user is able to compose these basic templates in a structured manner in order to form more complex queries. SEQUEL is intended as a data base sublanguage for both the professional programmer and the more infrequent data base user.},
  booktitle = {Proceedings of the 1974 ACM SIGFIDET (Now SIGMOD) Workshop on Data Description, Access and Control},
  pages = {249–264},
  numpages = {16},
  keywords = {Information Retrieval, Data Base Management Systems, Query Languages, Data Manipulation Languages},
  location = {Ann Arbor, Michigan},
  series = {SIGFIDET '74}
}
</pre>

<a name="10.1145/362384.362685"></a><pre>
@article{<a href="bibliography.html#10.1145/362384.362685">10.1145/362384.362685</a>,
  author = {Codd, E. F.},
  title = {A Relational Model of Data for Large Shared Data Banks},
  year = {1970},
  issue_date = {June 1970},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {13},
  number = {6},
  issn = {0001-0782},
  url = {https://doi.org/10.1145/362384.362685},
  doi = {10.1145/362384.362685},
  abstract = {Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation). A prompting service which supplies such information is not a satisfactory solution. Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation are changed. Changes in data representation will often be needed as a result of changes in query, update, and report traffic and natural growth in the types of stored information.Existing noninferential, formatted data systems provide users with tree-structured files or slightly more general network models of the data. In Section 1, inadequacies of these models are discussed. A model based on n-ary relations, a normal form for data base relations, and the concept of a universal data sublanguage are introduced. In Section 2, certain operations on relations (other than logical inference) are discussed and applied to the problems of redundancy and consistency in the user's model.},
  journal = {Commun. ACM},
  month = jun,
  pages = {377–387},
  numpages = {11},
  keywords = {composition, data base, redundancy, data structure, data bank, predicate calculus, retrieval language, relations, hierarchies of data, data organization, data integrity, consistency, networks of data, security, derivability, join}
}
</pre>

<a name="Liu2015ManagementOF"></a><pre>
@inproceedings{<a href="bibliography.html#Liu2015ManagementOF">Liu2015ManagementOF</a>,
  title = {Management of Flexible Schema Data in {RDBMSs} - Opportunities and Limitations for {NoSQL} -},
  author = {Zhen Hua Liu and Dieter Gawlick},
  booktitle = {Conference on Innovative Data Systems Research},
  year = {2015}
}
</pre>

<a name="10.14778/1687553.1687618"></a><pre>
@article{<a href="bibliography.html#10.14778/1687553.1687618">10.14778/1687553.1687618</a>,
  author = {Manegold, Stefan and Kersten, Martin L. and Boncz, Peter},
  title = {Database Architecture Evolution: Mammals Flourished Long before Dinosaurs Became Extinct},
  year = {2009},
  issue_date = {August 2009},
  publisher = {VLDB Endowment},
  volume = {2},
  number = {2},
  issn = {2150-8097},
  url = {https://doi.org/10.14778/1687553.1687618},
  doi = {10.14778/1687553.1687618},
  abstract = {The holy grail for database architecture research is to find a solution that is Scalable &amp; Speedy, to run on anything from small ARM processors up to globally distributed compute clusters, Stable &amp; Secure, to service a broad user community, Small &amp; Simple, to be comprehensible to a small team of programmers, Self-managing, to let it run out-of-the-box without hassle.In this paper, we provide a trip report on this quest, covering both past experiences, ongoing research on hardware-conscious algorithms, and novel ways towards self-management specifically focused on column store solutions.},
  journal = {Proc. VLDB Endow.},
  month = aug,
  pages = {1648–1653},
  numpages = {6}
}
</pre>

<a name="10.1145/1376616.1376645"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/1376616.1376645">10.1145/1376616.1376645</a>,
  author = {Brantner, Matthias and Florescu, Daniela and Graf, David and Kossmann, Donald and Kraska, Tim},
  title = {Building a Database on {S3}},
  year = {2008},
  isbn = {9781605581026},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1376616.1376645},
  doi = {10.1145/1376616.1376645},
  abstract = {There has been a great deal of hype about Amazon's simple storage service (S3). S3 provides infinite scalability and high availability at low cost. Currently, S3 is used mostly to store multi-media documents (videos, photos, audio) which are shared by a community of people and rarely updated. The purpose of this paper is to demonstrate the opportunities and limitations of using S3 as a storage system for general-purpose database applications which involve small objects and frequent updates. Read, write, and commit protocols are presented. Furthermore, the cost (\$), performance, and consistency properties of such a storage system are studied.},
  booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
  pages = {251–264},
  numpages = {14},
  keywords = {ec2, database, simpledb, cost trade-off, aws, storage system, s3, concurrency, sqs, performance, cloud computing, eventual consistency},
  location = {Vancouver, Canada},
  series = {SIGMOD '08}
}
</pre>

<a name="10.1145/3373376.3378504"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/3373376.3378504">10.1145/3373376.3378504</a>,
  author = {Bindschaedler, Laurent and Goel, Ashvin and Zwaenepoel, Willy},
  title = {Hailstorm: Disaggregated Compute and Storage for Distributed {LSM}-Based Databases},
  year = {2020},
  isbn = {9781450371025},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3373376.3378504},
  doi = {10.1145/3373376.3378504},
  abstract = {Distributed LSM-based databases face throughput and latency issues due to load imbalance across instances and interference from background tasks such as flushing, compaction, and data migration. Hailstorm addresses these problems by deploying the database storage engines over a distributed filesystem that disaggregates storage from processing, enabling storage pooling and compaction offloading. Hailstorm pools storage devices within a rack, allowing each storage engine to fully utilize the aggregate rack storage capacity and bandwidth. Storage pooling successfully handles load imbalance without the need for resharding. Hailstorm offloads compaction tasks to remote nodes, distributing their impact, and improving overall system throughput and response time. We show that Hailstorm achieves load balance in many MongoDB deployments with skewed workloads, improving the average throughput by 60\%, while decreasing tail latency by as much as 5X. In workloads with range queries, Hailstorm provides up to 22X throughput improvements. Hailstorm also enables cost savings of 47-56\% in OLTP workloads.},
  booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages = {301–316},
  numpages = {16},
  keywords = {rocksdb, storage, ycsb, key-value store, disaggregation, mongodb, compaction offloading, tpc-e, hailstorm, tikv, database, skew, tidb, tpc-c, compute, distributed},
  location = {Lausanne, Switzerland},
  series = {ASPLOS '20}
}
</pre>

<a name="10.1016/S0306-4379(03)00047-4"></a><pre>
@article{<a href="bibliography.html#10.1016/S0306-4379(03)00047-4">10.1016/S0306-4379(03)00047-4</a>,
  author = {Torp, Kristian and Jensen, Christian S. and Snodgrass, Richard T.},
  title = {Modification Semantics in Now-Relative Databases},
  year = {2004},
  issue_date = {December 2004},
  publisher = {Elsevier Science Ltd.},
  address = {GBR},
  volume = {29},
  number = {8},
  issn = {0306-4379},
  url = {https://doi.org/10.1016/S0306-4379(03)00047-4},
  doi = {10.1016/S0306-4379(03)00047-4},
  abstract = {Most real-world databases record time-varying information. In such databases, the notion of "the current time," or NOW, occurs naturally and prominently. For example, when capturing the past states of a relation using begin and end time columns, tuples that are part of the current state have some past time as their begin time and NOW as their end time. While the semantics of such variable databases has been described in detail and is well understood, the modification of variable databases remains unexplored. This paper defines the semantics of modifications involving the variable NOW. More specifically, the problems with modifications in the presence of NOW are explored, illustrating that the main problems are with modifications of tuples that reach into the future. The paper defines the semantics of modifications--including insertions, deletions, and updates--of databases without NOW, with NOW, and with values of the type NOW + Δ, where Δ is a non-variable time duration. To accommodate these semantics, three new timestamp values are introduced. Finally, implementation is explored. We show how to represent the variable NOW with columns of standard SQL data types and give a mapping from SQL on NOW-relative data to standard SQL on these columns. The paper thereby completes the semantics, the querying, and the modification of now-relative databases.},
  journal = {Inf. Syst.},
  month = dec,
  pages = {653–683},
  numpages = {31},
  keywords = {updates, temporal query language, temporal data, SQL, Now-relative information, Now}
}
</pre>

<a name="10.1145/22952.22956"></a><pre>
@article{<a href="bibliography.html#10.1145/22952.22956">10.1145/22952.22956</a>,
  author = {Snodgrass, Richard},
  title = {The Temporal Query Language {TQuel}},
  year = {1987},
  issue_date = {June 1987},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {12},
  number = {2},
  issn = {0362-5915},
  url = {https://doi.org/10.1145/22952.22956},
  doi = {10.1145/22952.22956},
  abstract = {Recently, attention has been focused on temporal databases, representing an enterprise over time. We have developed a new language, Tquel, to query a temporal database. TQuel was designed to be a minimal extension, both syntactically and semantically, of Quel, the query language in the Ingres relational database management system. This paper discusses the language informally, then provides a tuple relational calculus semantics for the TQuel statements that differ from their Quel counterparts, including the modification statements. The three additional temporal constructs defined in Tquel are shown to be direct semantic analogues of Quel's where clause and target list. We also discuss reducibility of the semantics to Quel's semantics when applied to a static database. TQuel is compared with ten other query languages supporting time.},
  journal = {ACM Trans. Database Syst.},
  month = jun,
  pages = {247–298},
  numpages = {52}
}
</pre>

<a name="DBLP:conf/btw/0001K15"></a><pre>
@inproceedings{<a href="bibliography.html#DBLP:conf/btw/0001K15">DBLP:conf/btw/0001K15</a>,
  author = {Neumann, Thomas and Kemper, Alfons},
  title = {Unnesting Arbitrary Queries},
  booktitle = {Datenbanksysteme f{\"{u}}r Business, Technologie und Web (BTW),
               16. Fachtagung des GI-Fachbereichs "Datenbanken und Informationssysteme"
               (DBIS), 4.-6.3.2015 in Hamburg, Germany. Proceedings},
  series = {{LNI}},
  volume = {{P-241}},
  pages = {383--402},
  publisher = {{GI}},
  year = {2015},
  url = {https://dl.gi.de/20.500.12116/2418},
  timestamp = {Thu, 14 Nov 2019 16:35:26 +0100},
  biburl = {https://dblp.org/rec/conf/btw/0001K15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
</pre>

<a name="10.1145/3183713.3196931"></a><pre>
@inproceedings{<a href="bibliography.html#10.1145/3183713.3196931">10.1145/3183713.3196931</a>,
  author = {Zhang, Huanchen and Lim, Hyeontaek and Leis, Viktor and Andersen, David G. and Kaminsky, Michael and Keeton, Kimberly and Pavlo, Andrew},
  title = {{SuRF:} Practical Range Query Filtering with Fast Succinct Tries},
  year = {2018},
  isbn = {9781450347037},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3183713.3196931},
  doi = {10.1145/3183713.3196931},
  abstract = {We present the Succinct Range Filter (SuRF), a fast and compact data structure for approximate membership tests. Unlike traditional Bloom filters, SuRF supports both single-key lookups and common range queries: open-range queries, closed-range queries, and range counts. SuRF is based on a new data structure called the Fast Succinct Trie (FST) that matches the point and range query performance of state-of-the-art order-preserving indexes, while consuming only 10 bits per trie node. The false positive rates in SuRF for both point and range queries are tunable to satisfy different application needs. We evaluate SuRF in RocksDB as a replacement for its Bloom filters to reduce I/O by filtering requests before they access on-disk data structures. Our experiments on a 100 GB dataset show that replacing RocksDB's Bloom filters with SuRFs speeds up open-seek (without upper-bound) and closed-seek (with upper-bound) queries by up to 1.5\texttimes{} and 5\texttimes{} with a modest cost on the worst-case (all-missing) point query throughput due to slightly higher false positive rate.},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  pages = {323–336},
  numpages = {14},
  keywords = {fast succinct tries, lsm-trees, range filter, succinct data structures, surf},
  location = {Houston, TX, USA},
  series = {SIGMOD '18}
}
</pre>

<a name="10.1007/978-3-642-03784-9_3"></a><pre>
@inproceedings{<a href="bibliography.html#10.1007/978-3-642-03784-9_3">10.1007/978-3-642-03784-9_3</a>,
  author = {Brisaboa, Nieves and Ladra, Susana and Navarro, Gonzalo},
  year = {2009},
  month = aug,
  pages = {18-30},
  title = {k2-Trees for Compact Web Graph Representation},
  booktitle = {String Processing and Information Retrieval},
  isbn = {978-3-642-03783-2},
  journal = {Lecture Notes in Computer Science},
  doi = {10.1007/978-3-642-03784-9_3}
}
</pre>

<a name="10.1145/351827.351829"></a><pre>
@article{<a href="bibliography.html#10.1145/351827.351829">10.1145/351827.351829</a>,
  author = {Eppstein, David},
  title = {Fast Hierarchical Clustering and Other Applications of Dynamic Closest Pairs},
  year = {2001},
  issue_date = {2000},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {5},
  issn = {1084-6654},
  url = {https://doi.org/10.1145/351827.351829},
  doi = {10.1145/351827.351829},
  abstract = {We develop data structures for dynamic closest pair problems with arbitrary distance functions, that do not necessarily come from any geometric structure on the objects. Based on a technique previously used by the author for Euclidean closest pairs, we show how to insert and delete objects from an <i>n</i>-object set, maintaining the closest pair, in <i>O</i>(<i>n</i> log<sup>2</sup> <i>n</i>) time per update and <i>O</i>(<i>n</i>) space. With quadratic space, we can instead use a quadtree-like structure to achieve an optimal time bound, <i>O</i>(<i>n</i>) per update. We apply these data structures to hierarchical clustering, greedy matching, and TSP heuristics, and discuss other potential applications in machine learning, Gr\"{o}bner bases, and local improvement algorithms for partition and placement problems. Experiments show our new methods to be faster in practice than previously used heuristics.},
  journal = {ACM J. Exp. Algorithmics},
  month = dec,
  pages = {1–es},
  numpages = {23},
  keywords = {nearest-neighbor heuristic, matching, quadtree, conga line data structure, TSP}
}
</pre>

<a name="doi:10.1137/1.9781611976014.6"></a><pre>
@inbook{<a href="bibliography.html#doi:10.1137/1.9781611976014.6">doi:10.1137/1.9781611976014.6</a>,
  author = {Chan, Timothy M.},
  title = {Dynamic Generalized Closest Pair: Revisiting {Eppstein's} Technique},
  booktitle = {Symposium on Simplicity in Algorithms (SOSA)},
  year = {2020},
  publisher = {Society for Industrial and Applied Mathematics},
  chapter = {},
  pages = {33-37},
  doi = {10.1137/1.9781611976014.6},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976014.6},
  eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611976014.6}
}
</pre>

<a name="10.1145/202660.202667"></a><pre>
@article{<a href="bibliography.html#10.1145/202660.202667">10.1145/202660.202667</a>,
  author = {Darwen, Hugh and Date, C. J.},
  title = {The Third Manifesto},
  year = {1995},
  issue_date = {March 1995},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {24},
  number = {1},
  issn = {0163-5808},
  url = {https://doi.org/10.1145/202660.202667},
  doi = {10.1145/202660.202667},
  abstract = {We present a manifesto for the future direction of data and database management systems. The manifesto consists of a series of prescriptions, proscriptions, and "very strong suggestions."},
  journal = {SIGMOD Rec.},
  month = mar,
  pages = {39–49},
  numpages = {11}
}
</pre>

<a name="978-1-4493-7332-0"></a><pre>
@book{<a href="bibliography.html#978-1-4493-7332-0">978-1-4493-7332-0</a>,
  abstract = {Data is at the center of many challenges in system design today. Difficult issues need to be figured out, such as scalability, consistency, reliability, efficiency, and maintainability. In addition, we have an overwhelming variety of tools, including relational databases, NoSQL datastores, stream or batch processors, and message brokers. What are the right choices for your application? How do you make sense of all these buzzwords? In this practical and comprehensive guide, author Martin Kleppmann helps you navigate this diverse landscape by examining the pros and cons of various technologies for processing and storing data. Software keeps changing, but the fundamental principles remain the same.},
  author = {Kleppmann, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/24ee7900592839fd42b02b0836724e3ae/flint63},
  file = {eBook:2017/Kleppmann17.pdf:PDF;O'Reilly Product page:http\://shop.oreilly.com/product/0636920032175.do:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/1449373321/:URL},
  groups = {public},
  interhash = {35b14b82d95d9ac5fd25b7a1b3e22ea0},
  intrahash = {4ee7900592839fd42b02b0836724e3ae},
  isbn = {978-1-4493-7332-0},
  keywords = {01841 103 safari book software development database architecture},
  publisher = {O'Reilly},
  timestamp = {2018-04-16T11:37:30.000+0200},
  title = {Designing Data-Intensive Applications},
  url = {https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/},
  username = {flint63},
  year = {2016}
}
</pre>

<a name="CMU-fall2020"></a><pre>
@misc{<a href="bibliography.html#CMU-fall2020">CMU-fall2020</a>,
  author = {Andy Pavlo},
  title = {Introduction to Database Systems},
  url = {https://15445.courses.cs.cmu.edu/fall2020/schedule.html},
  publisher = {Carnegie Mellon University School of Computer Science},
  year = {2020}
}
</pre>

<a name="CMU-spring2020"></a><pre>
@misc{<a href="bibliography.html#CMU-spring2020">CMU-spring2020</a>,
  author = {Andy Pavlo},
  title = {Advanced Database Systems},
  url = {https://15721.courses.cs.cmu.edu/spring2020/schedule.html},
  publisher = {Carnegie Mellon University School of Computer Science},
  year = {2020}
}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.99.</em></p>
</body>
</html>
